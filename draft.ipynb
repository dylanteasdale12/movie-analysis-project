{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import sqlite3\n",
    "from pandasql import sqldf\n",
    "pysqldf = lambda q: sqldf(q, globals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below was provided by Abhineet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_files = glob(\"./zippedData/*.csv.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {}\n",
    "for file in csv_files:\n",
    "    d[file] = pd.read_csv(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_files_dict = {}\n",
    "for filename in csv_files:\n",
    "    filename_cleaned = os.path.basename(filename).replace(\".csv\", \"\").replace(\".\", \"_\") # cleaning the filenames\n",
    "    filename_df = pd.read_csv(filename, index_col=0)\n",
    "    csv_files_dict[filename_cleaned] = filename_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(\"movies_db.sqlite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sql_table_from_df(df, name, conn):\n",
    "    try:\n",
    "        df.to_sql(name, conn)\n",
    "        print(f\"Created table {name}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"could not make table {name}\")\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "could not make table bom_movie_gross_gz\n",
      "Table 'bom_movie_gross_gz' already exists.\n",
      "could not make table imdb_name_basics_gz\n",
      "Table 'imdb_name_basics_gz' already exists.\n",
      "could not make table imdb_title_akas_gz\n",
      "Table 'imdb_title_akas_gz' already exists.\n",
      "could not make table imdb_title_basics_gz\n",
      "Table 'imdb_title_basics_gz' already exists.\n",
      "could not make table imdb_title_crew_gz\n",
      "Table 'imdb_title_crew_gz' already exists.\n",
      "could not make table imdb_title_principals_gz\n",
      "Table 'imdb_title_principals_gz' already exists.\n",
      "could not make table imdb_title_ratings_gz\n",
      "Table 'imdb_title_ratings_gz' already exists.\n",
      "could not make table tmdb_movies_gz\n",
      "Table 'tmdb_movies_gz' already exists.\n",
      "could not make table tn_movie_budgets_gz\n",
      "Table 'tn_movie_budgets_gz' already exists.\n"
     ]
    }
   ],
   "source": [
    "for name, table in csv_files_dict.items():\n",
    "    create_sql_table_from_df(table, name, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('bom_movie_gross_gz',),\n",
       " ('imdb_name_basics_gz',),\n",
       " ('imdb_title_akas_gz',),\n",
       " ('imdb_title_basics_gz',),\n",
       " ('imdb_title_crew_gz',),\n",
       " ('imdb_title_principals_gz',),\n",
       " ('imdb_title_ratings_gz',),\n",
       " ('tmdb_movies_gz',),\n",
       " ('tn_movie_budgets_gz',)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conn.execute(\"select name from sqlite_master where type='table';\").fetchall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we start our own code\n",
    "\n",
    "Since we will be working with Pandas we need to organize our DFs. \n",
    "Let's start by renaming all of them and adding them to a new dictionary. \n",
    "By creating a new dictionary of DFs, we can manipulate the data without messing up the originals in csv_files_dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_repository = {} #by creating a dictionary, we can manipulate the data without messing up the originals in csv_files_dict\n",
    "for filename in csv_files:\n",
    "    filename_cleaned = os.path.basename(filename).replace(\".csv\", \"\").replace(\".\", \"_\") # cleaning the filenames    \n",
    "    exec('df_'+ filename_cleaned + \"\"\" = csv_files_dict['\"\"\" + filename_cleaned + \"\"\"']\"\"\") #executing code using strings\n",
    "    exec(\"\"\"df_repository[\"\"\" + \"\"\"'\"\"\" + filename_cleaned + \"\"\"']\"\"\" + \"\"\"= df_\"\"\" + filename_cleaned )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen below, the dataframes contain duplicates, which should be removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bom_movie_gross_gz contains  11  duplicates\n",
      "imdb_name_basics_gz contains  239  duplicates\n",
      "imdb_title_akas_gz contains  2220  duplicates\n",
      "imdb_title_basics_gz contains  122  duplicates\n",
      "imdb_title_crew_gz contains  22915  duplicates\n",
      "imdb_title_principals_gz contains  114187  duplicates\n",
      "imdb_title_ratings_gz contains  45074  duplicates\n",
      "tmdb_movies_gz contains  1020  duplicates\n",
      "tn_movie_budgets_gz contains  0  duplicates\n"
     ]
    }
   ],
   "source": [
    "for db in df_repository.keys():\n",
    "    print(db, 'contains ', df_repository[db].duplicated().sum(), ' duplicates')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_duplicates(dataframe): #returns a dataframe without duplicates\n",
    "    cleaned_filter = dataframe.duplicated()\n",
    "    index_to_drop = dataframe.loc[cleaned_filter].index\n",
    "    return dataframe.drop(index=index_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for db in df_repository.keys(): #loop to parse dataframes through the clean_duplicates() function we wrote above\n",
    "    df_repository[db] = clean_duplicates(df_repository[db])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bom_movie_gross_gz contains  0  duplicates\n",
      "imdb_name_basics_gz contains  0  duplicates\n",
      "imdb_title_akas_gz contains  0  duplicates\n",
      "imdb_title_basics_gz contains  0  duplicates\n",
      "imdb_title_crew_gz contains  0  duplicates\n",
      "imdb_title_principals_gz contains  0  duplicates\n",
      "imdb_title_ratings_gz contains  0  duplicates\n",
      "tmdb_movies_gz contains  0  duplicates\n",
      "tn_movie_budgets_gz contains  0  duplicates\n"
     ]
    }
   ],
   "source": [
    "for db in df_repository.keys():\n",
    "    print(db, 'contains ', df_repository[db].duplicated().sum(), ' duplicates') #checking if it worked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elimination of NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NaN_to_zero(DataFrame, DataSeries, change_into=0):\n",
    "    filt = DataSeries.isna()\n",
    "    DataFrame.loc[filt] = change_into\n",
    "    return DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "studio               5\n",
       "domestic_gross      28\n",
       "foreign_gross     1350\n",
       "year                 0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#testing the NaN_to_zero function \n",
    "df_bom_movie_gross_gz.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "studio             4\n",
       "domestic_gross    28\n",
       "foreign_gross      0\n",
       "year               0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NaN_to_zero(df_bom_movie_gross_gz, df_bom_movie_gross_gz.foreign_gross).isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know it works, we can start cleaning up NaN values. After looking through the DFs, we should find other numerical columns with NaN values, and then apply the function to them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some columns that are supposed to be numerical actually have data stored as strings. Let's write a function that transforms the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_to_float(string):\n",
    "    if string == None:\n",
    "        new_string = 0\n",
    "    else:\n",
    "        if (type(string) != int) and (type(string) != float):\n",
    "            new_string = string.replace('$', '')\n",
    "            new_string = new_string.replace(',', '')\n",
    "\n",
    "        else:\n",
    "            new_string = string\n",
    "    return float(new_string)\n",
    "\n",
    "\n",
    "def series_string_to_float(dataseries):\n",
    "    new_dataseries = dataseries.apply(string_to_float)\n",
    "    return new_dataseries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df_bom_movie_gross_gz.foreign_gross[0]) #the data here is stored as strings instead of numerical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.float64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_series = series_string_to_float(df_bom_movie_gross_gz.foreign_gross)\n",
    "type(new_series[0]) #checking if it works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After cleaning data, we can start creating new features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bom_movie_gross_gz.foreign_gross = series_string_to_float(df_bom_movie_gross_gz.foreign_gross)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>studio</th>\n",
       "      <th>domestic_gross</th>\n",
       "      <th>foreign_gross</th>\n",
       "      <th>year</th>\n",
       "      <th>total_gross</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>title</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Toy Story 3</td>\n",
       "      <td>BV</td>\n",
       "      <td>415000000.0</td>\n",
       "      <td>652000000.0</td>\n",
       "      <td>2010</td>\n",
       "      <td>1.067000e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Alice in Wonderland (2010)</td>\n",
       "      <td>BV</td>\n",
       "      <td>334200000.0</td>\n",
       "      <td>691300000.0</td>\n",
       "      <td>2010</td>\n",
       "      <td>1.025500e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Harry Potter and the Deathly Hallows Part 1</td>\n",
       "      <td>WB</td>\n",
       "      <td>296000000.0</td>\n",
       "      <td>664300000.0</td>\n",
       "      <td>2010</td>\n",
       "      <td>9.603000e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Inception</td>\n",
       "      <td>WB</td>\n",
       "      <td>292600000.0</td>\n",
       "      <td>535700000.0</td>\n",
       "      <td>2010</td>\n",
       "      <td>8.283000e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Shrek Forever After</td>\n",
       "      <td>P/DW</td>\n",
       "      <td>238700000.0</td>\n",
       "      <td>513900000.0</td>\n",
       "      <td>2010</td>\n",
       "      <td>7.526000e+08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            studio  domestic_gross  \\\n",
       "title                                                                \n",
       "Toy Story 3                                     BV     415000000.0   \n",
       "Alice in Wonderland (2010)                      BV     334200000.0   \n",
       "Harry Potter and the Deathly Hallows Part 1     WB     296000000.0   \n",
       "Inception                                       WB     292600000.0   \n",
       "Shrek Forever After                           P/DW     238700000.0   \n",
       "\n",
       "                                             foreign_gross  year   total_gross  \n",
       "title                                                                           \n",
       "Toy Story 3                                    652000000.0  2010  1.067000e+09  \n",
       "Alice in Wonderland (2010)                     691300000.0  2010  1.025500e+09  \n",
       "Harry Potter and the Deathly Hallows Part 1    664300000.0  2010  9.603000e+08  \n",
       "Inception                                      535700000.0  2010  8.283000e+08  \n",
       "Shrek Forever After                            513900000.0  2010  7.526000e+08  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bom_movie_gross_gz['total_gross'] = df_bom_movie_gross_gz['foreign_gross'].add(df_bom_movie_gross_gz['domestic_gross'], fill_value=0.0)\n",
    "df_bom_movie_gross_gz.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to simplify the manipulation of data, the column names of each df could be standardized ('title' vs 'movie title' vs 'original title', etc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
